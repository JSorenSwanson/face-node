**face\_nodes/** is broken into a group of services, each service orchestrated in a root project configuration manifest ```docker-compose.yml```. Each service is generated with either a cli tool or manually scripted. Dockerfile container environment manifests are located in the root directory for each service&#39;s root directory.

Services can be executed in containers from project root directory using docker-compose for development:<br/>
```docker-compose run {SERVICE}```

- **Frontend interface** (```frontend```) – _frontend\_web/_ Frontend application written in TS/Javascript. Scaffolded by vue cli (Vue.JS), which has descriptive dependencies (notably plugins/). The frontend service root project directory has some details on running the application in the readme and documentation on core Vue features used in the first iteration in the latest commit within the codebase. This application retrieves **state** from application services invoked via web requests to our API service ```flask```.
  - This service runs independently as configured in our docker-compose manifest.
- **API Server** (```flask```) – _flask\_rack/_ py http server which handles web requests and invokes server-side application services. Interfaces connection with Redis in-memory keystore database and utilizes JSON as a way to manage schemas for our key-val db. We manage application state in this service by receiving input from server-side application services.
  - This service requires ```redis``` service as a dependency.
- **Redis/DB** **(```redis```) – in memory key-val db which is built on an implementation of RedisJSON. We currently have a schema definition manually mapped in _flask\_rack/node\_schemas.py_ which represents our POC&#39;s sample definition of node settings utilized in the frontend service. Eventually, we&#39;ll utilize PSQL and store more manageable schemas derived from ORM (flask-sql-alchemy) to reduce schema mapping overhead.
  - This is a dependency of our ```flask``` compose service and runs with container durability
- **Server-side appln** (```eyebuffer```) – _eye\_buffer/_ ingests data from mjpeg/encoded stream and performs classification nearness using a serialized **.model** bin generated by ``` tf-train``` service. Output of detections are rendered to the host application over xserver for debugging purposes.
  - The ```eyebuffer``` service is decoupled from the rest of our service stack until we feed event data forward to/from our API server in ```flask``` service asynchronously.

- **Team-managed training service** (```tftrain```) – _tf\_train_ is a training environment used to generate the **.model** bin image that is used in the ```eyebuffer``` classification service – this service uses nvidia-docker toolkit ([https://github.com/NVIDIA/nvidia-docker](https://github.com/NVIDIA/nvidia-docker)) to automatically include the required nvidia libraries that tensorflow requires.
  - The ```tftrain``` service is decoupled from the rest of the service stack and has hardware resource definitions in the docker-compose manifest.
- **.env/.hostenv** – container shared environment variables.
- **LICENSE and README.md.old** **(~)** We maintain that our project&#39;s core application service leverages libraries and recognition strategy (caffe model, buffer strategy) from POC forked reference repository. NVIDIA DeepStream SDK implementation may not use this reference&#39;s assets in source but it should be crucial for us to document it, along with our other sources of inspiration and leverage, in a LICENSE citation source. One of us should compile our dependencies list and add it to our LICENSE. This should be done collaboratively and will allow us to agree on what we want to include in a broad sense.
